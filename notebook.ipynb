{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fasttext Supervised learning example \n",
    "\n",
    "This notebook is inspired by the [Supervised Learning fastText tutorial](https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read data 'line by line', using generators.\n",
    "    Generators make it easier to process BIG text files.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as input:\n",
    "        for line in input:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data(filename, data):\n",
    "    \"\"\"\n",
    "    Write result to a file.\n",
    "    \n",
    "    :param result: the list to be written to the file\n",
    "    \"\"\"\n",
    "    with open(filename, \"a\") as output:\n",
    "        output.write('{}\\n'.format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Preprocess data, filtering out stopwords, punctuation and lowering \n",
    "    all splitted tokens.\n",
    "    \n",
    "    :param data: the string data to be processed\n",
    "    \"\"\"    \n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        data = data.replace(char, ' ' + char + ' ')\n",
    "    sw = stopwords.words('english')\n",
    "    splitted_chunks = data.split()\n",
    "    lowered_chunks = (item.lower() for item in splitted_chunks)\n",
    "    chunks_without_punctuation = (chunk for chunk in lowered_chunks if chunk not in punctuation)\n",
    "    chunks_without_stopwords = (chunk for chunk in chunks_without_punctuation if chunk not in sw)\n",
    "    return \" \".join(list(chunks_without_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def pipeline(input_filename, output_filename, limit=None):\n",
    "    \"\"\"\n",
    "    Iterate over the rows and apply the text preprocessing.\n",
    "\n",
    "    :param input_filename: name of the input filename\n",
    "    :param output_filename: name of the output filename\n",
    "    :param limit: get the first N rows\n",
    "    \"\"\"    \n",
    "    open(output_filename, 'w').close()  # Hack to \"reset\" the output file\n",
    "    for row in islice(read_data(input_filename), 0, limit):\n",
    "        data = preprocess(row)\n",
    "        if data:\n",
    "            write_data(output_filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_data):\n",
    "    result = model.test(test_data)\n",
    "    print('Precision@1:', result.precision)\n",
    "    print('Recall@1:', result.recall)\n",
    "    print('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "data_dir = path.join(path.dirname(\"__file__\"), 'data')\n",
    "cooking_input = path.join(data_dir, 'cooking.train')\n",
    "cooking_input_norm = path.join(data_dir, 'cooking.train_norm')\n",
    "cooking_test = path.join(data_dir, 'cooking.test')\n",
    "cooking_test_norm = path.join(data_dir, 'cooking.test_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline(cooking_input, cooking_input_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "# Info to save the model\n",
    "model_dir = path.join(path.dirname(\"__file__\"), 'models')\n",
    "cooking_output = path.join(model_dir, 'cooking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not normalized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooking_model = ft.supervised(cooking_input, cooking_output, lr=1.0, epoch=10, silent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model(cooking_model, cooking_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooking_norm_model = ft.supervised(cooking_input_norm, cooking_output, lr=1.0, epoch=10, silent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline(cooking_test, cooking_test_norm)\n",
    "test_model(cooking_norm_model, cooking_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cooking_output_filename = path.join(current_dir, 'test', 'model_cooking.bin')\n",
    "# model = ft.load_model(cooking_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['Why not put knives in the dishwasher?']\n",
    "\n",
    "labels = cooking_model.predict(texts)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = cooking_norm_model.predict(texts)\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
